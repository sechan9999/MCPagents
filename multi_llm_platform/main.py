# main.py
"""
ë©€í‹° LLM í”Œë«í¼ ì‹¤í–‰
Multi-LLM Platform Execution

ë¹„ìš©ì„ ìµœì í™”í•˜ëŠ” ë©€í‹° LLM ë¼ìš°í„°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import asyncio

from config import MODEL_CONFIGS, TaskComplexity
from llm_router import LLMRouter, RouteStrategy
from performance_monitor import PerformanceMonitor, PerformanceMetric


async def run_demo():
    """ë°ëª¨ ì‹¤í–‰"""
    print("""
ğŸ’° Multi-LLM Platform - Smart Cost Optimization
================================================

Features:
âœ“ Task Complexity Analysis
âœ“ Cost-Optimized Routing
âœ“ Response Caching
âœ“ Performance Monitoring
""")

    # ë¼ìš°í„° ì´ˆê¸°í™”
    router = LLMRouter(
        strategy=RouteStrategy.BALANCED,
        use_cache=True,
        use_redis=False
    )
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
    monitor = PerformanceMonitor()
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        ("ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œìš”?", TaskComplexity.SIMPLE),
        ("íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.", TaskComplexity.COMPLEX),
        ("ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”.", TaskComplexity.MEDIUM),
        ("íšŒì‚¬ì˜ 5ë…„ íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.", TaskComplexity.CRITICAL),
    ]
    
    print("\nğŸ“Š Testing with different complexity levels...\n")
    
    for prompt, expected_complexity in test_prompts:
        print(f"{'='*60}")
        print(f"Prompt: {prompt[:50]}...")
        
        # ë¼ìš°íŒ… ì¶”ì²œ í™•ì¸
        recommendation = router.get_routing_recommendation(prompt)
        detected = recommendation["detected_complexity"]
        selected_model = recommendation["default_selection"]
        
        print(f"Detected Complexity: {detected}")
        print(f"Selected Model: {selected_model}")
        
        # ì‹¤í–‰
        result = await router.route_and_execute(prompt)
        
        print(f"Response (preview): {result['response'][:100]}...")
        print(f"Latency: {result['latency_ms']:.0f}ms")
        print(f"Cost: ${result['cost']:.6f}")
        print(f"Cached: {result['cached']}")
        
        # ì„±ëŠ¥ ê¸°ë¡
        monitor.record(PerformanceMetric(
            model=result["model"],
            latency_ms=result["latency_ms"],
            cost=result["cost"],
            success=True,
            complexity=TaskComplexity(detected),
            cached=result.get("cached", False)
        ))
        
        print()
    
    # ìºì‹œ í…ŒìŠ¤íŠ¸ (ë™ì¼ ì¿¼ë¦¬ ì¬ì‹¤í–‰)
    print(f"{'='*60}")
    print("Testing cache hit (repeating first query)...")
    result = await router.route_and_execute(test_prompts[0][0])
    print(f"Cached: {result['cached']}")
    print(f"Latency: {result['latency_ms']:.0f}ms")
    print(f"Cost: ${result['cost']:.6f}")
    print()
    
    # í†µê³„ ì¶œë ¥
    print(f"{'='*60}")
    print("ğŸ“ˆ Statistics")
    print(f"{'='*60}\n")
    
    stats = router.get_stats()
    
    print("Cost Summary:")
    print(f"  Total Cost: ${stats['cost']['total_cost']:.6f}")
    print(f"  Total Requests: {stats['cost']['total_requests']}")
    print(f"  Avg Cost/Request: ${stats['cost']['average_cost_per_request']:.6f}")
    
    if "cache" in stats:
        print(f"\nCache Stats:")
        print(f"  Hit Rate: {stats['cache'].get('hit_rate_percent', 'N/A')}")
        print(f"  Hits: {stats['cache'].get('hits', 0)}")
        print(f"  Misses: {stats['cache'].get('misses', 0)}")
    
    print("\n" + "="*60)
    print("ğŸ“Š Performance Report")
    print("="*60)
    print(monitor.generate_report())


def show_model_info():
    """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
    print("""
ğŸ“‹ Available Models
===================
""")
    
    for model_key, config in MODEL_CONFIGS.items():
        print(f"""
{model_key}:
  Provider: {config.provider.value}
  Model Name: {config.model_name}
  Input Cost: ${config.input_cost_per_1k}/1K tokens
  Output Cost: ${config.output_cost_per_1k}/1K tokens
  Max Tokens: {config.max_tokens}
  Avg Latency: {config.avg_latency_ms}ms
  Quality Score: {config.quality_score}/10
""")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Multi-LLM Platform - Smart Cost Optimization"
    )
    parser.add_argument(
        "--models",
        action="store_true",
        help="Show available models and pricing"
    )
    parser.add_argument(
        "--strategy",
        choices=["cost", "quality", "speed", "balanced"],
        default="balanced",
        help="Routing strategy"
    )
    
    args = parser.parse_args()
    
    if args.models:
        show_model_info()
    else:
        asyncio.run(run_demo())


if __name__ == "__main__":
    main()
