# semantic_cache.py
"""
Semantic Caching with Vector Database
ì‹œë§¨í‹± ìºì‹± - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©

ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¼ì¹˜ê°€ ì•„ë‹Œ ìœ ì‚¬ ì§ˆë¬¸ ê¸°ë°˜ ìºì‹±ìœ¼ë¡œ
ì‘ë‹µ ì†ë„ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
"""

from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import json
import time


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬ (Cache Entry)"""
    query: str
    response: Any
    embedding: List[float]
    model: str
    created_at: datetime
    access_count: int = 0
    last_accessed: datetime = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class EmbeddingProvider:
    """ì„ë² ë”© ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def embed(self, text: str) -> List[float]:
        raise NotImplementedError


class OpenAIEmbedding(EmbeddingProvider):
    """OpenAI ì„ë² ë”©"""
    
    def __init__(self, model: str = "text-embedding-3-small"):
        self.model = model
        self._client = None
        
    def _get_client(self):
        if self._client is None:
            try:
                from openai import OpenAI
                self._client = OpenAI()
            except ImportError:
                raise ImportError("pip install openai")
        return self._client
    
    def embed(self, text: str) -> List[float]:
        client = self._get_client()
        response = client.embeddings.create(
            model=self.model,
            input=text
        )
        return response.data[0].embedding


class SentenceTransformerEmbedding(EmbeddingProvider):
    """ë¡œì»¬ SentenceTransformer ì„ë² ë”© (ë¬´ë£Œ)"""
    
    def __init__(self, model: str = "all-MiniLM-L6-v2"):
        self.model_name = model
        self._model = None
    
    def _get_model(self):
        if self._model is None:
            try:
                from sentence_transformers import SentenceTransformer
                self._model = SentenceTransformer(self.model_name)
            except ImportError:
                raise ImportError("pip install sentence-transformers")
        return self._model
    
    def embed(self, text: str) -> List[float]:
        model = self._get_model()
        return model.encode(text).tolist()


class SimpleEmbedding(EmbeddingProvider):
    """ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜ ì„ë² ë”© (í´ë°±ìš©)"""
    
    def __init__(self, dim: int = 128):
        self.dim = dim
    
    def embed(self, text: str) -> List[float]:
        """í•´ì‹œ ê¸°ë°˜ ê°„ë‹¨í•œ ì„ë² ë”© ìƒì„±"""
        import hashlib
        
        # ë‹¨ì–´ë³„ í•´ì‹œ ìƒì„±
        words = text.lower().split()
        embedding = [0.0] * self.dim
        
        for word in words:
            h = int(hashlib.md5(word.encode()).hexdigest(), 16)
            for i in range(self.dim):
                embedding[i] += ((h >> i) & 1) * 2 - 1
        
        # ì •ê·œí™”
        magnitude = sum(x*x for x in embedding) ** 0.5
        if magnitude > 0:
            embedding = [x / magnitude for x in embedding]
        
        return embedding


class VectorStore:
    """ë²¡í„° ì €ì¥ì†Œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def add(self, id: str, embedding: List[float], metadata: Dict) -> None:
        raise NotImplementedError
    
    def search(self, embedding: List[float], top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        raise NotImplementedError
    
    def delete(self, id: str) -> bool:
        raise NotImplementedError


class ChromaVectorStore(VectorStore):
    """Chroma ë²¡í„° ì €ì¥ì†Œ"""
    
    def __init__(self, collection_name: str = "semantic_cache"):
        self.collection_name = collection_name
        self._client = None
        self._collection = None
    
    def _get_collection(self):
        if self._collection is None:
            try:
                import chromadb
                self._client = chromadb.Client()
                self._collection = self._client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"}
                )
            except ImportError:
                raise ImportError("pip install chromadb")
        return self._collection
    
    def add(self, id: str, embedding: List[float], metadata: Dict) -> None:
        collection = self._get_collection()
        collection.add(
            ids=[id],
            embeddings=[embedding],
            metadatas=[metadata]
        )
    
    def search(self, embedding: List[float], top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        collection = self._get_collection()
        results = collection.query(
            query_embeddings=[embedding],
            n_results=top_k,
            include=["distances", "metadatas"]
        )
        
        output = []
        if results["ids"] and results["ids"][0]:
            for i, id in enumerate(results["ids"][0]):
                distance = results["distances"][0][i] if results["distances"] else 0
                similarity = 1 - distance  # cosine distance to similarity
                metadata = results["metadatas"][0][i] if results["metadatas"] else {}
                output.append((id, similarity, metadata))
        
        return output
    
    def delete(self, id: str) -> bool:
        try:
            collection = self._get_collection()
            collection.delete(ids=[id])
            return True
        except:
            return False


class InMemoryVectorStore(VectorStore):
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    
    def __init__(self, max_size: int = 10000):
        self._store: Dict[str, Tuple[List[float], Dict]] = {}
        self._max_size = max_size
    
    def add(self, id: str, embedding: List[float], metadata: Dict) -> None:
        if len(self._store) >= self._max_size:
            # LRU: ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì‚­ì œ
            oldest = next(iter(self._store))
            del self._store[oldest]
        
        self._store[id] = (embedding, metadata)
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        dot = sum(x*y for x, y in zip(a, b))
        norm_a = sum(x*x for x in a) ** 0.5
        norm_b = sum(x*x for x in b) ** 0.5
        return dot / (norm_a * norm_b) if norm_a * norm_b > 0 else 0
    
    def search(self, embedding: List[float], top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        results = []
        for id, (stored_emb, metadata) in self._store.items():
            similarity = self._cosine_similarity(embedding, stored_emb)
            results.append((id, similarity, metadata))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def delete(self, id: str) -> bool:
        if id in self._store:
            del self._store[id]
            return True
        return False


class SemanticCache:
    """ì‹œë§¨í‹± ìºì‹œ (Semantic Cache)
    
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        embedding_provider: EmbeddingProvider = None,
        vector_store: VectorStore = None,
        similarity_threshold: float = 0.92,
        max_cache_size: int = 10000,
        ttl_hours: int = 24
    ):
        """ì‹œë§¨í‹± ìºì‹œ ì´ˆê¸°í™”
        
        Args:
            embedding_provider: ì„ë² ë”© ì œê³µì
            vector_store: ë²¡í„° ì €ì¥ì†Œ
            similarity_threshold: ìºì‹œ íˆíŠ¸ ì„ê³„ê°’ (0.0~1.0)
            max_cache_size: ìµœëŒ€ ìºì‹œ í¬ê¸°
            ttl_hours: ìºì‹œ ìœ íš¨ ì‹œê°„
        """
        self.embedding_provider = embedding_provider or SimpleEmbedding()
        self.vector_store = vector_store or InMemoryVectorStore(max_cache_size)
        self.similarity_threshold = similarity_threshold
        self.ttl_hours = ttl_hours
        
        # ìºì‹œ ë°ì´í„° ì €ì¥ì†Œ
        self._cache_data: Dict[str, CacheEntry] = {}
        
        # í†µê³„
        self._stats = {
            "hits": 0,
            "misses": 0,
            "semantic_hits": 0,  # ìœ ì‚¬ ì§ˆë¬¸ íˆíŠ¸
            "exact_hits": 0,     # ì •í™• ì¼ì¹˜ íˆíŠ¸
            "total_saved_cost": 0.0,
            "total_saved_latency_ms": 0
        }
    
    def _generate_id(self, query: str, model: str) -> str:
        """ìºì‹œ ID ìƒì„±"""
        content = f"{model}:{query}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    def get(
        self,
        query: str,
        model: str = None,
        check_semantic: bool = True
    ) -> Optional[Tuple[Any, float, str]]:
        """ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ
        
        Args:
            query: ì¿¼ë¦¬ ë¬¸ìì—´
            model: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ ëª¨ë“  ëª¨ë¸)
            check_semantic: ì‹œë§¨í‹± ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Optional[Tuple]: (ì‘ë‹µ, ìœ ì‚¬ë„, íˆíŠ¸ íƒ€ì…) ë˜ëŠ” None
        """
        # 1. ì •í™• ì¼ì¹˜ í™•ì¸
        cache_id = self._generate_id(query, model or "any")
        if cache_id in self._cache_data:
            entry = self._cache_data[cache_id]
            if self._is_valid(entry):
                entry.access_count += 1
                entry.last_accessed = datetime.now()
                self._stats["hits"] += 1
                self._stats["exact_hits"] += 1
                return entry.response, 1.0, "exact"
        
        # 2. ì‹œë§¨í‹± ê²€ìƒ‰
        if check_semantic:
            try:
                query_embedding = self.embedding_provider.embed(query)
                results = self.vector_store.search(query_embedding, top_k=3)
                
                for cached_id, similarity, metadata in results:
                    if similarity >= self.similarity_threshold:
                        # ëª¨ë¸ í•„í„°
                        if model and metadata.get("model") != model:
                            continue
                        
                        if cached_id in self._cache_data:
                            entry = self._cache_data[cached_id]
                            if self._is_valid(entry):
                                entry.access_count += 1
                                entry.last_accessed = datetime.now()
                                self._stats["hits"] += 1
                                self._stats["semantic_hits"] += 1
                                return entry.response, similarity, "semantic"
            except Exception as e:
                print(f"âš ï¸ Semantic search error: {e}")
        
        self._stats["misses"] += 1
        return None
    
    def set(
        self,
        query: str,
        response: Any,
        model: str,
        estimated_cost: float = 0,
        latency_ms: int = 0,
        metadata: Dict = None
    ) -> str:
        """ì‘ë‹µ ìºì‹±
        
        Args:
            query: ì¿¼ë¦¬ ë¬¸ìì—´
            response: ì‘ë‹µ ë°ì´í„°
            model: ëª¨ë¸ ì´ë¦„
            estimated_cost: ì¶”ì • ë¹„ìš©
            latency_ms: ë ˆì´í„´ì‹œ (ë°€ë¦¬ì´ˆ)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            str: ìºì‹œ ID
        """
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_provider.embed(query)
            
            cache_id = self._generate_id(query, model)
            
            # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
            entry = CacheEntry(
                query=query,
                response=response,
                embedding=embedding,
                model=model,
                created_at=datetime.now(),
                last_accessed=datetime.now(),
                metadata={
                    "estimated_cost": estimated_cost,
                    "latency_ms": latency_ms,
                    **(metadata or {})
                }
            )
            
            self._cache_data[cache_id] = entry
            
            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            self.vector_store.add(
                id=cache_id,
                embedding=embedding,
                metadata={
                    "model": model,
                    "query_preview": query[:100],
                    "created_at": entry.created_at.isoformat()
                }
            )
            
            return cache_id
            
        except Exception as e:
            print(f"âš ï¸ Cache set error: {e}")
            return ""
    
    def _is_valid(self, entry: CacheEntry) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸"""
        age_hours = (datetime.now() - entry.created_at).total_seconds() / 3600
        return age_hours < self.ttl_hours
    
    def invalidate(self, query: str, model: str = None) -> bool:
        """ìºì‹œ ë¬´íš¨í™”"""
        cache_id = self._generate_id(query, model or "any")
        if cache_id in self._cache_data:
            del self._cache_data[cache_id]
            self.vector_store.delete(cache_id)
            return True
        return False
    
    def clear(self):
        """ì „ì²´ ìºì‹œ ì´ˆê¸°í™”"""
        self._cache_data.clear()
        self._stats = {k: 0 for k in self._stats}
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        total = self._stats["hits"] + self._stats["misses"]
        hit_rate = self._stats["hits"] / total if total > 0 else 0
        
        return {
            **self._stats,
            "total_requests": total,
            "hit_rate": hit_rate,
            "hit_rate_percent": f"{hit_rate * 100:.1f}%",
            "cache_size": len(self._cache_data),
            "semantic_hit_ratio": (
                self._stats["semantic_hits"] / self._stats["hits"]
                if self._stats["hits"] > 0 else 0
            )
        }


# === í…ŒìŠ¤íŠ¸ ===
def test_semantic_cache():
    """ì‹œë§¨í‹± ìºì‹œ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ§  Semantic Cache Test")
    print("=" * 60)
    
    cache = SemanticCache(
        similarity_threshold=0.85
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    queries = [
        ("What is machine learning?", "gpt-4"),
        ("Explain machine learning to me", "gpt-4"),  # ìœ ì‚¬
        ("What's ML?", "gpt-4"),  # ìœ ì‚¬
        ("How does Python work?", "gpt-4"),  # ë‹¤ë¦„
        ("What is machine learning?", "gpt-4"),  # ì •í™• ì¼ì¹˜
    ]
    
    # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ìºì‹±
    print("\nğŸ“ Caching first query...")
    cache.set(
        query=queries[0][0],
        response={"answer": "Machine learning is a subset of AI..."},
        model=queries[0][1],
        estimated_cost=0.002,
        latency_ms=1500
    )
    
    # ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    for query, model in queries:
        result = cache.get(query, model)
        if result:
            response, similarity, hit_type = result
            print(f"\nâœ… HIT ({hit_type})")
            print(f"   Query: {query[:40]}...")
            print(f"   Similarity: {similarity:.2%}")
        else:
            print(f"\nâŒ MISS")
            print(f"   Query: {query[:40]}...")
    
    # í†µê³„
    print("\n" + "=" * 60)
    print("ğŸ“Š Cache Statistics")
    print("=" * 60)
    stats = cache.get_stats()
    print(f"   Hit Rate: {stats['hit_rate_percent']}")
    print(f"   Semantic Hits: {stats['semantic_hits']}")
    print(f"   Exact Hits: {stats['exact_hits']}")
    print(f"   Misses: {stats['misses']}")
    
    print("\nâœ… Semantic Cache Test Complete!")


if __name__ == "__main__":
    test_semantic_cache()
